#!/usr/bin/env python3
"""Newbie æ¨¡å‹è½¬ Diffusers æ ¼å¼å·¥å…·"""

import argparse
import json
import os
import sys
from pathlib import Path

import torch
from safetensors.torch import save_file
from transformers import AutoModel, AutoTokenizer
from diffusers import AutoencoderKL

sys.path.insert(0, str(Path(__file__).parent.parent))
import models


def parse_args():
    parser = argparse.ArgumentParser(description="å°†Newbie (3B CLIP)æ¨¡å‹è½¬æ¢ä¸ºDiffusersæ ¼å¼")
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="æ¨¡å‹checkpointçš„.pthæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: consolidated.00-of-01.pth)"
    )
    parser.add_argument(
        "--gemma3",
        type=str,
        default="google/gemma-3-4b-it",
        help="Gemma3æ¨¡å‹çš„HuggingFace repoæˆ–æœ¬åœ°è·¯å¾„ (é»˜è®¤: google/gemma-3-4b-it)"
    )
    parser.add_argument(
        "--jina",
        type=str,
        default="jinaai/jina-clip-v2",
        help="Jina CLIPæ¨¡å‹çš„HuggingFace repoæˆ–æœ¬åœ°è·¯å¾„ (é»˜è®¤: jinaai/jina-clip-v2)"
    )
    parser.add_argument(
        "--vae",
        type=str,
        default="black-forest-labs/FLUX.1-dev",
        help="VAEæ¨¡å‹çš„HuggingFace repoæˆ–æœ¬åœ°è·¯å¾„ (é»˜è®¤: black-forest-labs/FLUX.1-devï¼Œä½¿ç”¨å…¶ä¸­çš„VAEå­æ–‡ä»¶å¤¹)"
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="bf16",
        choices=["bf16", "fp16", "fp32"],
        help="æ¨¡å‹ç²¾åº¦ (é»˜è®¤: bf16)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ä¸checkpointåŒç›®å½•ä¸‹çš„diffusersæ–‡ä»¶å¤¹)"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="NextDiT_3B_GQA_patch2_Adaln_Refiner_WHIT_CLIP",
        help="æ¨¡å‹åç§° (é»˜è®¤: NextDiT_3B_GQA_patch2_Adaln_Refiner_WHIT_CLIP)"
    )
    parser.add_argument(
        "--hf_token",
        type=str,
        default=None,
        help="HuggingFace tokenç”¨äºä¸‹è½½å—é™æ¨¡å‹"
    )
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="æ˜¯å¦å°†è½¬æ¢åçš„æ¨¡å‹æ¨é€åˆ°HuggingFace Hub"
    )
    parser.add_argument(
        "--hub_repo_id",
        type=str,
        default=None,
        help="HuggingFace Hub repo ID (ä¾‹å¦‚: username/newbie-diffusers)"
    )

    return parser.parse_args()


def get_dtype(dtype_str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºtorch dtype"""
    dtype_map = {
        "bf16": torch.bfloat16,
        "fp16": torch.float16,
        "fp32": torch.float32
    }
    return dtype_map[dtype_str]


def load_checkpoint(checkpoint_path):
    """åŠ è½½æ¨¡å‹checkpoint"""
    print(f"\n{'='*60}")
    print("ğŸ“‚ åŠ è½½Checkpoint")
    print(f"{'='*60}")
    print(f"è·¯å¾„: {checkpoint_path}")

    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location="cpu")

    # æå–state_dict
    if isinstance(checkpoint, dict):
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
            print("âœ… ä»'model'é”®åŠ è½½state_dict")
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            print("âœ… ä»'state_dict'é”®åŠ è½½state_dict")
        else:
            state_dict = checkpoint
            print("âœ… ç›´æ¥ä½¿ç”¨checkpointä½œä¸ºstate_dict")
    else:
        state_dict = checkpoint
        print("âœ… ç›´æ¥ä½¿ç”¨checkpointä½œä¸ºstate_dict")

    print(f"ğŸ“Š å‚æ•°æ€»æ•°: {len(state_dict)}")

    return state_dict


def load_text_encoder(gemma3_path, dtype, hf_token=None):
    """åŠ è½½Gemma3æ–‡æœ¬ç¼–ç å™¨"""
    print(f"\n{'='*60}")
    print("ğŸ“ åŠ è½½Gemma3æ–‡æœ¬ç¼–ç å™¨")
    print(f"{'='*60}")
    print(f"è·¯å¾„: {gemma3_path}")
    print(f"ç²¾åº¦: {dtype}")

    text_encoder = AutoModel.from_pretrained(
        gemma3_path,
        torch_dtype=dtype,
        token=hf_token
    )

    tokenizer = AutoTokenizer.from_pretrained(
        gemma3_path,
        token=hf_token
    )
    tokenizer.padding_side = "right"

    cap_feat_dim = text_encoder.config.text_config.hidden_size
    print(f"âœ… Gemma3åŠ è½½æˆåŠŸ")
    print(f"ğŸ“Š éšè—ç»´åº¦: {cap_feat_dim}")

    return text_encoder, tokenizer, cap_feat_dim


def load_clip_model(jina_path, dtype, hf_token=None):
    """åŠ è½½Jina CLIPæ¨¡å‹"""
    print(f"\n{'='*60}")
    print("ğŸ–¼ï¸  åŠ è½½Jina CLIPæ¨¡å‹")
    print(f"{'='*60}")
    print(f"è·¯å¾„: {jina_path}")
    print(f"ç²¾åº¦: {dtype}")

    from transformers import AutoConfig

    clip_config = AutoConfig.from_pretrained(
        jina_path,
        trust_remote_code=True
    )

    clip_tokenizer = AutoTokenizer.from_pretrained(
        jina_path,
        trust_remote_code=True
    )

    clip_model = AutoModel.from_pretrained(
        jina_path,
        torch_dtype=dtype,
        config=clip_config,
        trust_remote_code=True,
        token=hf_token
    )

    print(f"âœ… Jina CLIPåŠ è½½æˆåŠŸ")

    return clip_model, clip_tokenizer, clip_config


def load_vae(vae_path, dtype, hf_token=None):
    """åŠ è½½VAEæ¨¡å‹"""
    print(f"\n{'='*60}")
    print("ğŸ¨ åŠ è½½VAEæ¨¡å‹")
    print(f"{'='*60}")
    print(f"è·¯å¾„: {vae_path}")
    print(f"ç²¾åº¦: {dtype}")

    # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«vaeå­æ–‡ä»¶å¤¹çš„å®Œæ•´æ¨¡å‹è·¯å¾„ï¼ˆå¦‚FLUXï¼‰
    vae_subfolder_path = os.path.join(vae_path, "vae") if os.path.isdir(vae_path) else None

    if vae_subfolder_path and os.path.exists(os.path.join(vae_subfolder_path, "config.json")):
        print(f"æ£€æµ‹åˆ°VAEå­æ–‡ä»¶å¤¹ï¼Œä» {vae_subfolder_path} åŠ è½½")
        vae = AutoencoderKL.from_pretrained(
            vae_subfolder_path,
            torch_dtype=dtype,
            token=hf_token
        )
    else:
        vae = AutoencoderKL.from_pretrained(
            vae_path,
            subfolder="vae" if not vae_path.endswith("vae") else None,
            torch_dtype=dtype,
            token=hf_token
        )

    print(f"âœ… VAEåŠ è½½æˆåŠŸ")
    print(f"ğŸ“Š æ½œåœ¨ç©ºé—´é€šé“æ•°: {vae.config.latent_channels}")

    return vae


def create_model(model_name, cap_feat_dim, qk_norm=True):
    """åˆ›å»ºNextDiT CLIPæ¨¡å‹"""
    print(f"\n{'='*60}")
    print("ğŸ—ï¸  åˆ›å»ºNextDiT CLIPæ¨¡å‹")
    print(f"{'='*60}")
    print(f"æ¨¡å‹: {model_name}")
    print(f"cap_feat_dim: {cap_feat_dim}")
    print(f"qk_norm: {qk_norm}")

    if model_name not in models.__dict__:
        raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨æ¨¡å‹: {list(models.__dict__.keys())}")

    model = models.__dict__[model_name](
        in_channels=16,
        qk_norm=qk_norm,
        cap_feat_dim=cap_feat_dim,
    )

    param_count = model.parameter_count()
    print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“Š å‚æ•°é‡: {param_count:,}")

    return model


def convert_to_diffusers(
    model,
    text_encoder,
    tokenizer,
    clip_model,
    clip_tokenizer,
    vae,
    output_dir,
    dtype
):
    """å°†æ¨¡å‹è½¬æ¢ä¸ºDiffusersæ ¼å¼"""
    print(f"\n{'='*60}")
    print("ğŸ”„ è½¬æ¢ä¸ºDiffusersæ ¼å¼")
    print(f"{'='*60}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # 1. ä¿å­˜DiTæ¨¡å‹
    dit_path = output_path / "transformer"
    dit_path.mkdir(exist_ok=True)

    print("\nğŸ“¦ ä¿å­˜DiT Transformer...")
    # ä¿å­˜ä¸ºsafetensorsæ ¼å¼
    model_state_dict = model.state_dict()
    save_file(model_state_dict, dit_path / "diffusion_pytorch_model.safetensors")

    # ä¿å­˜æ¨¡å‹é…ç½®
    model_config = {
        "model_type": "nextdit_clip",
        "patch_size": 2,
        "dim": 2304,
        "n_layers": 36,
        "n_heads": 24,
        "n_kv_heads": 8,
        "in_channels": 16,
        "axes_dims": [32, 32, 32],
        "axes_lens": [1024, 512, 512],
        "clip_text_dim": 1024,
        "clip_img_dim": 1024,
        "_class_name": "NextDiT_3B_GQA_patch2_Adaln_Refiner_WHIT_CLIP",
    }

    with open(dit_path / "config.json", "w") as f:
        json.dump(model_config, f, indent=2)

    print(f"âœ… DiTæ¨¡å‹å·²ä¿å­˜åˆ°: {dit_path}")

    # 2. ä¿å­˜æ–‡æœ¬ç¼–ç å™¨ (Gemma3)
    text_encoder_path = output_path / "text_encoder"
    text_encoder_path.mkdir(exist_ok=True)

    print("\nğŸ“ ä¿å­˜Gemma3æ–‡æœ¬ç¼–ç å™¨...")
    text_encoder.save_pretrained(text_encoder_path, safe_serialization=True)
    tokenizer.save_pretrained(text_encoder_path)
    print(f"âœ… Gemma3å·²ä¿å­˜åˆ°: {text_encoder_path}")

    # 3. ä¿å­˜CLIPæ¨¡å‹
    clip_path = output_path / "clip_model"
    clip_path.mkdir(exist_ok=True)

    print("\nğŸ–¼ï¸  ä¿å­˜Jina CLIPæ¨¡å‹...")
    clip_model.save_pretrained(clip_path, safe_serialization=True)
    clip_tokenizer.save_pretrained(clip_path)
    print(f"âœ… Jina CLIPå·²ä¿å­˜åˆ°: {clip_path}")

    # 4. ä¿å­˜VAE
    vae_path = output_path / "vae"
    vae_path.mkdir(exist_ok=True)

    print("\nğŸ¨ ä¿å­˜VAE...")
    vae.save_pretrained(vae_path, safe_serialization=True)
    print(f"âœ… VAEå·²ä¿å­˜åˆ°: {vae_path}")

    # 5. åˆ›å»ºæ¨¡å‹ç´¢å¼•å’Œé…ç½®
    model_index = {
        "_class_name": "NewbiePipeline",
        "_diffusers_version": "0.30.0",
        "transformer": ["transformer", "NextDiT_3B_GQA_patch2_Adaln_Refiner_WHIT_CLIP"],
        "text_encoder": ["text_encoder", "Gemma3ForConditionalGeneration"],
        "tokenizer": ["text_encoder", "AutoTokenizer"],
        "clip_model": ["clip_model", "JinaCLIPModel"],
        "clip_tokenizer": ["clip_model", "AutoTokenizer"],
        "vae": ["vae", "AutoencoderKL"],
    }

    with open(output_path / "model_index.json", "w") as f:
        json.dump(model_index, f, indent=2)

    print(f"\nâœ… æ¨¡å‹ç´¢å¼•å·²åˆ›å»º")

    # 5. åˆ›å»ºREADME
    readme_content = f"""---
license: apache-2.0
tags:
- text-to-image
- diffusion
- nextdit
- lumina
library_name: diffusers
pipeline_tag: text-to-image
---

# Newbie (3B CLIP) - Diffusers Format

è¿™æ˜¯Newbie AIçš„NextDiT 3B CLIPæ¨¡å‹çš„Diffusersæ ¼å¼ç‰ˆæœ¬ã€‚

## æ¨¡å‹æ¶æ„

- **Transformer**: NextDiT (3Bå‚æ•°)
  - 36å±‚ Transformer blocks
  - 24ä¸ªæ³¨æ„åŠ›å¤´ï¼Œ8ä¸ªKVå¤´ (GQA)
  - éšè—ç»´åº¦: 2304
  - Patch size: 2

- **æ–‡æœ¬ç¼–ç å™¨**: Gemma-3-4B-IT
  - ç”¨äºæå–æ–‡æœ¬ç‰¹å¾å’Œæ¡ä»¶ç”Ÿæˆ

- **CLIPæ¨¡å‹**: Jina CLIP v2
  - æä¾›é¢å¤–çš„æ–‡æœ¬-å›¾åƒå¯¹é½ç‰¹å¾
  - å¢å¼ºæ–‡æœ¬ç†è§£èƒ½åŠ›

## ä½¿ç”¨æ–¹æ³•

```python
import torch
from diffusers import DiffusionPipeline

# åŠ è½½ç®¡é“
pipe = DiffusionPipeline.from_pretrained(
    "path/to/this/model",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)
pipe = pipe.to("cuda")

# ç”Ÿæˆå›¾åƒ
prompt = "ä¸€åªå¯çˆ±çš„çŒ«å’ªåœ¨èŠ±å›­é‡Œç©è€"
image = pipe(prompt).images[0]
image.save("output.png")
```

## æ¨¡å‹å‚æ•°

- ç²¾åº¦: {dtype}
- è¾“å…¥é€šé“: 16 (VAEæ½œåœ¨ç©ºé—´)
- ä½ç½®ç¼–ç : 3è½´RoPE
- å½’ä¸€åŒ–: RMSNorm + QK Norm

## è®¸å¯è¯

Apache 2.0

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œè¯·å¼•ç”¨Newbie AIé¡¹ç›®ã€‚
"""

    with open(output_path / "README.md", "w") as f:
        f.write(readme_content)

    print(f"\nâœ… READMEå·²åˆ›å»º")

    print(f"\n{'='*60}")
    print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nç›®å½•ç»“æ„:")
    print(f"  {output_dir}/")
    print(f"  â”œâ”€â”€ transformer/")
    print(f"  â”‚   â”œâ”€â”€ diffusion_pytorch_model.safetensors")
    print(f"  â”‚   â””â”€â”€ config.json")
    print(f"  â”œâ”€â”€ text_encoder/")
    print(f"  â”‚   â”œâ”€â”€ model.safetensors")
    print(f"  â”‚   â”œâ”€â”€ config.json")
    print(f"  â”‚   â””â”€â”€ tokenizer files...")
    print(f"  â”œâ”€â”€ clip_model/")
    print(f"  â”‚   â”œâ”€â”€ model.safetensors")
    print(f"  â”‚   â”œâ”€â”€ config.json")
    print(f"  â”‚   â””â”€â”€ tokenizer files...")
    print(f"  â”œâ”€â”€ vae/")
    print(f"  â”‚   â”œâ”€â”€ diffusion_pytorch_model.safetensors")
    print(f"  â”‚   â””â”€â”€ config.json")
    print(f"  â”œâ”€â”€ model_index.json")
    print(f"  â””â”€â”€ README.md")

    return output_path


def push_to_hub(output_dir, repo_id, hf_token):
    """æ¨é€åˆ°HuggingFace Hub"""
    print(f"\n{'='*60}")
    print("â˜ï¸  æ¨é€åˆ°HuggingFace Hub")
    print(f"{'='*60}")
    print(f"Repo ID: {repo_id}")

    try:
        from huggingface_hub import HfApi, create_repo

        api = HfApi()

        # åˆ›å»ºä»“åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        try:
            create_repo(repo_id, token=hf_token, exist_ok=True)
            print(f"âœ… ä»“åº“å·²åˆ›å»º/ç¡®è®¤: {repo_id}")
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºä»“åº“æ—¶å‡ºç°è­¦å‘Š: {e}")

        # ä¸Šä¼ æ–‡ä»¶
        print("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶ä¸­...")
        api.upload_folder(
            folder_path=output_dir,
            repo_id=repo_id,
            repo_type="model",
            token=hf_token
        )

        print(f"âœ… æˆåŠŸæ¨é€åˆ°: https://huggingface.co/{repo_id}")

    except ImportError:
        print("âŒ éœ€è¦å®‰è£…huggingface_hub: pip install huggingface_hub")
        return False
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")
        return False

    return True


def main():
    args = parse_args()

    print("=" * 60)
    print("ğŸš€ Newbie (3B CLIP) è½¬ Diffusers æ ¼å¼")
    print("=" * 60)

    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output is None:
        checkpoint_dir = Path(args.checkpoint).parent
        output_dir = checkpoint_dir / "diffusers"
    else:
        output_dir = Path(args.output)

    # è·å–dtype
    dtype = get_dtype(args.dtype)
    print(f"\né…ç½®:")
    print(f"  Checkpoint: {args.checkpoint}")
    print(f"  Gemma3: {args.gemma3}")
    print(f"  Jina CLIP: {args.jina}")
    print(f"  VAE: {args.vae}")
    print(f"  ç²¾åº¦: {args.dtype}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")

    # 1. åŠ è½½checkpoint
    state_dict = load_checkpoint(args.checkpoint)

    # 2. åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    text_encoder, tokenizer, cap_feat_dim = load_text_encoder(
        args.gemma3, dtype, args.hf_token
    )

    # 3. åŠ è½½CLIPæ¨¡å‹
    clip_model, clip_tokenizer, clip_config = load_clip_model(
        args.jina, dtype, args.hf_token
    )

    # 4. åŠ è½½VAE
    vae = load_vae(args.vae, dtype, args.hf_token)

    # 5. åˆ›å»ºDiTæ¨¡å‹
    model = create_model(args.model_name, cap_feat_dim)

    # 6. åŠ è½½æƒé‡
    print(f"\n{'='*60}")
    print("âš™ï¸  åŠ è½½æ¨¡å‹æƒé‡")
    print(f"{'='*60}")

    try:
        model.load_state_dict(state_dict, strict=True)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸ (strict=True)")
    except Exception as e:
        print(f"âš ï¸  ä¸¥æ ¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éä¸¥æ ¼åŠ è½½...")
        print(f"é”™è¯¯: {e}")
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print(f"âš ï¸  ç¼ºå¤±çš„é”®: {len(missing_keys)}")
        print(f"âš ï¸  æ„å¤–çš„é”®: {len(unexpected_keys)}")
        if missing_keys:
            print(f"ç¼ºå¤±é”®ç¤ºä¾‹: {missing_keys[:5]}")
        if unexpected_keys:
            print(f"æ„å¤–é”®ç¤ºä¾‹: {unexpected_keys[:5]}")

    # è½¬æ¢ä¸ºæŒ‡å®šç²¾åº¦
    model = model.to(dtype)

    # 7. è½¬æ¢ä¸ºDiffusersæ ¼å¼
    output_path = convert_to_diffusers(
        model=model,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        clip_model=clip_model,
        clip_tokenizer=clip_tokenizer,
        vae=vae,
        output_dir=output_dir,
        dtype=args.dtype
    )

    # 8. æ¨é€åˆ°Hubï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.push_to_hub:
        if args.hub_repo_id is None:
            print("\nâŒ é”™è¯¯: éœ€è¦æŒ‡å®š --hub_repo_id æ‰èƒ½æ¨é€åˆ°Hub")
        else:
            push_to_hub(output_path, args.hub_repo_id, args.hf_token)

    print(f"\n{'='*60}")
    print("âœ¨ å…¨éƒ¨å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"1. æµ‹è¯•è½¬æ¢åçš„æ¨¡å‹:")
    print(f"   cd {output_path}")
    print(f"   python -c \"from diffusers import DiffusionPipeline; pipe = DiffusionPipeline.from_pretrained('.', trust_remote_code=True)\"")
    print(f"\n2. æ¨é€åˆ°HuggingFace Hub (å¯é€‰):")
    print(f"   python {__file__} --checkpoint {args.checkpoint} --push_to_hub --hub_repo_id your-username/newbie-diffusers")


if __name__ == "__main__":
    main()
